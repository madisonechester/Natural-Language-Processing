{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributions: Dafni's Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout our collaborative project, my contributions focused on refining key components of our pipeline, particularly in text processing and feature engineering. My efforts were driven by a desire to optimize various aspects of our project to ensure its effectiveness.\n",
    "\n",
    "\n",
    "I took the lead in enhancing text vectorization techniques, particularly refining TF-IDF and Count Vectorization with multiple bindings. These enhancements were critical for transforming our textual data into numerical representations, a crucial step in the development of our machine learning models. Additionally, I developed and integrated preprocessing functions such as punctuation removal and custom stop word elimination to enhance the relevance and quality of our input data.\n",
    "\n",
    "\n",
    "A significant portion of my work was dedicated to optimizing the cosine similarity function, a vital metric for evaluating the similarity between our textual documents. I focused on addressing inefficiencies in handling sparse matrix outputs, which were essential for seamless integration with other encoded features within our model. Although I explored the potential of using Cython for further optimization, the constraints of our pilot model limited its implementation at that stage.\n",
    "\n",
    "\n",
    "Furthermore, I led the innovation of additional features aimed at enriching our training dataset. These included:\n",
    "\n",
    "\n",
    "- Coincident Keywords: A novel approach to quantifying question similarity by identifying common question keywords like \"What,\" \"Where,\" or \"Which.\" This feature leveraged the insight that similar questions often share similar question stems.\n",
    "\n",
    "\n",
    "- Jaccard Distance: A sophisticated metric for measuring similarity between two sets based on the ratio of their intersection to their union. In our context, it provided nuanced insights into the similarity between question strings based on their respective word sets.\n",
    "\n",
    "\n",
    "- Levenshtein Distance: An algorithmic measure of string similarity, accounting for the minimum number of single-character edits required to transform one string into another. I explored variations of this distance metric, including word-level and character-level comparisons, despite encountering memory constraints with the latter.\n",
    "\n",
    "\n",
    "Additionally, I delved into the utilization of Word Mover's Distance (WMD), an advanced technique for measuring semantic similarity between documents using word embeddings. By leveraging cutting-edge embedding techniques like word2vec and GloVe, WMD facilitated a deeper understanding of textual semantics. However, its implementation required an external set of words to establish semantic metrics, introducing additional computational overhead.\n",
    "\n",
    "\n",
    "In summary, my contributions were instrumental in advancing various technical domains within our project, from text processing and feature engineering to the integration of advanced similarity metrics. My expertise and innovation laid a solid foundation for extracting meaningful insights from our textual data, driving our project towards its objectives.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
