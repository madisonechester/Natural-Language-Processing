{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Basic Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:32:38.496531Z",
     "start_time": "2023-02-22T16:32:31.431877Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "from typing import Iterable\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:32:38.503126Z",
     "start_time": "2023-02-22T16:32:38.499623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.__version__=1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(f'sklearn.__version__={sklearn.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:33:09.276088Z",
     "start_time": "2023-02-22T16:33:09.272616Z"
    }
   },
   "outputs": [],
   "source": [
    "# spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter    = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "# nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:33:11.674799Z",
     "start_time": "2023-02-22T16:33:11.667250Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                PorterStemmer       LancasterStemmer    \n",
      "\n",
      "dogs                dog                 dog                 \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n",
      "pass                pass                pass                \n",
      "passing             pass                pass                \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "passed              pass                pass                \n",
      "trouble             troubl              troubl              \n",
      "troubling           troubl              troubl              \n",
      "care                care                car                 \n",
      "believes            believ              believ              \n"
     ]
    }
   ],
   "source": [
    "words = [\"dogs\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\n",
    "         \"football\",\"pass\",\"passing\",\"friendship\", \"friends\", \"friendships\",\n",
    "         \"passed\",\"trouble\",\"troubling\",\"care\", \"believes\"]\n",
    "preprocess = [porter, lancaster]\n",
    "\n",
    "len_bin = 20\n",
    "col_formater = \"{0:len_bin}{1:len_bin}{2:len_bin}\".replace(\"len_bin\",str(len_bin))\n",
    "print(col_formater.format(\"Word\", porter.__class__.__name__, lancaster.__class__.__name__))\n",
    "print(\"\")\n",
    "for w in words:\n",
    "    print( col_formater.format(w, porter.stem(w), lancaster.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:40:13.735208Z",
     "start_time": "2022-02-23T16:40:13.730729Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def stem(sentence):\n",
    "    token_words = word_tokenize(sentence)\n",
    "    sentence_stemmed = []\n",
    "    for word in token_words:\n",
    "        sentence_stemmed.append(porter.stem(word))\n",
    "        sentence_stemmed.append(\" \")\n",
    "    return \"\".join(sentence_stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:40:14.169873Z",
     "start_time": "2022-02-23T16:40:14.165343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'j.k. rowl wrote harri potter . she never expect the book to be famou . '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"J.K. Rowling wrote Harry Potter. She never expected the book to be famous.\"\n",
    "stem(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T16:35:16.898070Z",
     "start_time": "2021-03-01T16:35:16.893744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J.K. Rowling wrote Harry Potter.',\n",
       " 'She never expected the book to be famous.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:09.482102Z",
     "start_time": "2021-03-01T12:00:09.479536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J',\n",
       " 'K',\n",
       " ' Rowling wrote Harry Potter',\n",
       " ' She never expected the book to be famous',\n",
       " '']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Be carefull separating phrases\n",
    "s.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:09.487212Z",
     "start_time": "2021-03-01T12:00:09.484157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J.K.',\n",
       " 'Rowling',\n",
       " 'wrote',\n",
       " 'Harry',\n",
       " 'Potter',\n",
       " '.',\n",
       " 'She',\n",
       " 'never',\n",
       " 'expected',\n",
       " 'the',\n",
       " 'book',\n",
       " 'to',\n",
       " 'be',\n",
       " 'famous',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Lemmatization\n",
    "\n",
    "\n",
    "Lemmatization consists on properly use of a vocabulary and morphological analysis of words, aiming to remove inflectional endings only with the goal of returning any word to a set of base (or dictionary form) words.\n",
    "\n",
    "\n",
    "`Lemmatize(saw) = see`\n",
    "\n",
    "\n",
    "We will use a lemmatizer from WordNet (https://wordnet.princeton.edu) avaliable from nltk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:40:42.197929Z",
     "start_time": "2022-02-23T16:40:42.193460Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:40:55.586339Z",
     "start_time": "2022-02-23T16:40:55.581486Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"I was running and eating. This was a terrible idea.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:40:56.025899Z",
     "start_time": "2022-02-23T16:40:55.999563Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "I                   I                   \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "This                This                \n",
      "was                 wa                  \n",
      "a                   a                   \n",
      "terrible            terrible            \n",
      "idea                idea                \n"
     ]
    }
   ],
   "source": [
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the words did no change!\n",
    "\n",
    "This is because there was no context. If we give a part of speech type then the lemmatizer will do what we would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T16:37:19.701471Z",
     "start_time": "2021-03-01T16:37:19.698730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "I                   I                   \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "This                This                \n",
      "was                 be                  \n",
      "a                   a                   \n",
      "terrible            terrible            \n",
      "idea                idea                \n"
     ]
    }
   ],
   "source": [
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T16:37:27.522658Z",
     "start_time": "2021-03-01T16:37:27.517302Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                PorterStemmer       LancasterStemmer    WordNetLemmatizer   \n",
      "\n",
      "dogs                dog                 dog                 dog                 \n",
      "destabilize         destabil            dest                destabilize         \n",
      "misunderstanding    misunderstand       misunderstand       misunderstanding    \n",
      "railroad            railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             football            \n",
      "pass                pass                pass                pas                 \n",
      "passing             pass                pass                passing             \n",
      "friendship          friendship          friend              friendship          \n",
      "friends             friend              friend              friend              \n",
      "friendships         friendship          friend              friendship          \n",
      "passed              pass                pass                passed              \n",
      "trouble             troubl              troubl              trouble             \n",
      "troubling           troubl              troubl              troubling           \n",
      "care                care                car                 care                \n",
      "believes            believ              believ              belief              \n"
     ]
    }
   ],
   "source": [
    "words = [\"dogs\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\n",
    "         \"football\",\"pass\",\"passing\",\"friendship\", \"friends\", \"friendships\",\n",
    "         \"passed\",\"trouble\",\"troubling\",\"care\", \"believes\"]\n",
    "preprocess = [porter, lancaster, wordnet_lemmatizer]\n",
    "\n",
    "len_bin = 20\n",
    "col_formater = \"{0:len_bin}{1:len_bin}{2:len_bin}{3:len_bin}\".replace(\"len_bin\",str(len_bin))\n",
    "print(col_formater.format(\"Word\", porter.__class__.__name__, lancaster.__class__.__name__, wordnet_lemmatizer.__class__.__name__))\n",
    "print(\"\")\n",
    "for w in words:\n",
    "    print( col_formater.format(w, porter.stem(w), lancaster.stem(w), wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Features for documents\n",
    "\n",
    "\n",
    "### From docs to feature vectors: Make your own countvectorizer\n",
    "\n",
    "\n",
    "Let us build a simple document classifier featurizing each document by word counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:34:31.365925Z",
     "start_time": "2023-02-22T16:34:31.359038Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.datasets\n",
    "import scipy\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:34:35.498431Z",
     "start_time": "2023-02-22T16:34:34.244104Z"
    }
   },
   "outputs": [],
   "source": [
    "X = sklearn.datasets.fetch_20newsgroups()\n",
    "\n",
    "X_train = sklearn.datasets.fetch_20newsgroups(subset=\"train\").data\n",
    "y_train = sklearn.datasets.fetch_20newsgroups(subset=\"train\").target\n",
    "X_test  = sklearn.datasets.fetch_20newsgroups(subset=\"test\").data\n",
    "y_test  = sklearn.datasets.fetch_20newsgroups(subset=\"test\").target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:34:46.343463Z",
     "start_time": "2023-02-22T16:34:46.339969Z"
    }
   },
   "outputs": [],
   "source": [
    "x = X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:43:06.035635Z",
     "start_time": "2022-02-23T16:43:06.028683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:42:37.383950Z",
     "start_time": "2022-02-23T16:42:37.379237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:11.545231Z",
     "start_time": "2021-03-01T12:00:11.542259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiny function to create a feature matrix of word counts (feature counting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 2, 1, 1]\n",
      "[0, 1, 2, 3, 4]\n",
      "[0, 2, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0, 0, 0],\n",
       "       [0, 0, 2, 1, 1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "docs = [['hello', 'world', 'hello'], ['goodbye', 'cruel', 'teacher', 'goodbye']]\n",
    "\n",
    "def prepare_word_counts_with_dict(docs: Iterable[str], verbose=False):\n",
    "    ind_ptr = [0]\n",
    "    ind_col = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "    \n",
    "    for m, doc in enumerate(docs):\n",
    "        word_ind_counter = defaultdict(int)  # document counter for each doc in X\n",
    "        for word in doc: \n",
    "            vocabulary.setdefault(word, len(vocabulary))\n",
    "            word_ind_counter[word] += 1\n",
    "                \n",
    "        data.extend(word_ind_counter.values())\n",
    "        ind_ptr.append(ind_ptr[-1] + len(word_ind_counter))\n",
    "        ind_col.extend([vocabulary[w] for w in word_ind_counter.keys()])\n",
    "    \n",
    "    if verbose:\n",
    "        print(data)\n",
    "        print(ind_col)\n",
    "        print(ind_ptr)\n",
    "    return (data, ind_col, ind_ptr)\n",
    "\n",
    "sp.csr_matrix(prepare_word_counts_with_dict(docs, verbose=True)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [['hello', 'world', 'hello'], ['goodbye', 'cruel', 'teacher', 'goodbye']]\n",
    "\n",
    "def prepare_word_counts_with_dict(docs: Iterable[str], verbose=False):\n",
    "    ind_ptr = [0]\n",
    "    ind_col = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "    \n",
    "    for m, doc in enumerate(docs):\n",
    "        word_ind_counter = defaultdict(int)  # document counter for each doc in X\n",
    "        for word in doc: \n",
    "            vocabulary.setdefault(word, len(vocabulary))\n",
    "            word_ind_counter[word] += 1\n",
    "                \n",
    "        data.extend(word_ind_counter.values())\n",
    "        ind_ptr.append(ind_ptr[-1] + len(word_ind_counter))\n",
    "        ind_col.extend([vocabulary[w] for w in word_ind_counter.keys()])\n",
    "\n",
    "    if verbose:\n",
    "        print('len vocab =', len(vocabulary))\n",
    "        print('vocab =', vocabulary)\n",
    "        print('data =', data)\n",
    "        print('ind_ptr =', ind_ptr)\n",
    "        print('ind_col =', ind_col)\n",
    "        \n",
    "    return (data, ind_col, ind_ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 1, 2, 1, 1], [0, 1, 2, 3, 4], [0, 2, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_word_counts_with_dict(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0, 0, 0],\n",
       "       [0, 0, 2, 1, 1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.csr_matrix(prepare_word_counts_with_dict(docs)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a bigger dataset to benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_big = docs * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.45 ms Â± 467 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "prepare_word_counts_with_dict(docs_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x5 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 5000 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.csr_matrix(prepare_word_counts_with_dict(docs_big))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiny function to create a feature matrix of word counts  (no feature counting)\n",
    "\n",
    "We can create a CSR word count matrix without explicitly counting each word count.\n",
    "\n",
    "- **Note**: `sp.csr_matrix` is smart enough to join counts of `data`, `ind_col` and `ind_ptr` that happen to be in the same coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [['hello', 'world', 'hello'], ['goodbye', 'cruel', 'teacher']]\n",
    "\n",
    "def prepare_word_counts0(docs: Iterable[str]):\n",
    "    ind_ptr = [0]\n",
    "    ind_col = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "    \n",
    "    for doc in docs:\n",
    "        for w in doc:\n",
    "            if w in vocabulary:\n",
    "                index = vocabulary[w]\n",
    "            else:\n",
    "                index = len(vocabulary)\n",
    "                vocabulary[w] = len(vocabulary)\n",
    "            ind_col.append(index)\n",
    "            data.append(1)\n",
    "        ind_ptr.append(len(ind_col))\n",
    "    return (data, ind_col, ind_ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1]\n",
      "[0, 1, 0, 2, 3, 4]\n",
      "[0, 3, 6]\n"
     ]
    }
   ],
   "source": [
    "data, ind_col, ind_ptr = prepare_word_counts0(docs)\n",
    "print(data)\n",
    "print(ind_col)\n",
    "print(ind_ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.csr_matrix(prepare_word_counts0(docs)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "docs = [['hello', 'world', 'hello'], ['goodbye', 'cruel', 'teacher']]\n",
    "\n",
    "def prepare_word_counts1(docs: Iterable[str]):\n",
    "    ind_ptr = [0]\n",
    "    ind_col = []\n",
    "    data = []\n",
    "    vocabulary = defaultdict(int)\n",
    "    \n",
    "    for doc in docs:\n",
    "        for w in doc:\n",
    "            index = vocabulary.setdefault(w, len(vocabulary))\n",
    "            ind_col.append(index)\n",
    "            data.append(1)\n",
    "        ind_ptr.append(len(ind_col))\n",
    "    \n",
    "    return (data, ind_col, ind_ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.csr_matrix(prepare_word_counts1(docs)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmarking  approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_big = docs*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.69 ms Â± 185 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "prepare_word_counts0(docs_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23 ms Â± 737 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "prepare_word_counts1(docs_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53 ms Â± 581 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "prepare_word_counts_with_dict(docs_big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customising Vectorizer classes\n",
    "\n",
    "- **preprocessor**: a callable that takes an entire document as input (as a single string), and returns a possibly transformed version of the document, still as an entire string. This can be used to remove HTML tags, lowercase the entire document, etc.\n",
    "\n",
    "\n",
    "- **tokenizer**: a callable that takes the output from the preprocessor and splits it into tokens, then returns a list of these.\n",
    "\n",
    "\n",
    "- **analyzer**: a callable that replaces the preprocessor and tokenizer. The default analyzers all call the preprocessor and tokenizer, but custom analyzers will skip this. N-gram extraction and stop word filtering take place at the analyzer level, so a custom analyzer may have to reproduce these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of how to encode sparse matrix fast\n",
    "\n",
    "\n",
    "Notice that in order to build our data as a matrix we need to use sparse matrices due to the high dimensionality (number of words/features) of the vocabulary.\n",
    "\n",
    "Here there is a little example to illustrate how we can build a csr_matrix (compressed sparse row matrix) fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:44:11.920221Z",
     "start_time": "2022-02-23T16:44:11.902757Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 2],\n",
       "       [2, 1, 0],\n",
       "       [0, 1, 3]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1,0,2],[2,1,0],[0,1,3]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:44:12.339149Z",
     "start_time": "2022-02-23T16:44:12.332451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 2],\n",
       "        [2, 1, 0],\n",
       "        [0, 1, 3]], dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,2,1,1,3]\n",
    "row  = [0,0,1,1,2,2]\n",
    "#ind_ptr = [0,3,4,9]\n",
    "col = [0,2,0,1,1,2]\n",
    "sp.csr_matrix( (data,(row,col)), shape=(3,3) ).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Build a Simple countvectorizer\n",
    "\n",
    "Complete methods `fit` and `transform`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:44:15.377045Z",
     "start_time": "2022-02-23T16:44:15.372815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('From: jcm@head-cfa.harvard.edu (Jonathan McDowell)\\nSubject: Re: Shuttle Launch Question\\nOrganization: Smithsonian Astrophysical Observatory, Cambridge, MA,  USA\\nDistribution: sci\\nLines: 23\\n\\nFrom article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\\n>>In article <C5JLwx.4H9.1@cs.cmu.edu>, ETRAT@ttacs1.ttu.edu (Pack Rat) writes...\\n>>>\"Clear caution & warning memory.  Verify no unexpected\\n>>>errors. ...\".  I am wondering what an \"expected error\" might\\n>>>be.  Sorry if this is a really dumb question, but\\n> \\n> Parity errors in memory or previously known conditions that were waivered.\\n>    \"Yes that is an error, but we already knew about it\"\\n> I\\'d be curious as to what the real meaning of the quote is.\\n> \\n> tom\\n\\n\\nMy understanding is that the \\'expected errors\\' are basically\\nknown bugs in the warning system software - things are checked\\nthat don\\'t have the right values in yet because they aren\\'t\\nset till after launch, and suchlike. Rather than fix the code\\nand possibly introduce new bugs, they just tell the crew\\n\\'ok, if you see a warning no. 213 before liftoff, ignore it\\'.\\n\\n - Jonathan\\n\\n\\n',\n",
       " 14)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[4], y_train[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:11.585220Z",
     "start_time": "2021-03-01T12:00:11.583601Z"
    }
   },
   "outputs": [],
   "source": [
    "# \"David's car\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T16:52:20.013586Z",
     "start_time": "2022-02-23T16:52:19.991490Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.sparse as sp\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from collections import defaultdict\n",
    "import re\n",
    "stemmer =  SnowballStemmer(language='english')\n",
    "\n",
    "class SimpleCountVectorizer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 min_word_counts=1,\n",
    "                 doc_cleaner_pattern=r\"[^a-zA-Z]\",\n",
    "                 token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 dtype=np.float32,\n",
    "                 doc_cleaner_func=None,\n",
    "                 tokenizer_func=None,\n",
    "                 word_transformer_func=None):\n",
    "        \n",
    "        self._retype = type(re.compile('hello, world'))\n",
    "\n",
    "        self.min_word_counts     = min_word_counts\n",
    "        self.doc_cleaner_pattern = doc_cleaner_pattern\n",
    "        self.token_pattern       = token_pattern\n",
    "        self.dtype               = dtype\n",
    "        \n",
    "        self.doc_cleaner_func      = doc_cleaner_func\n",
    "        self.tokenizer_func        = tokenizer_func\n",
    "        self.word_transformer_func = word_transformer_func\n",
    "\n",
    "        self.word_to_pos = {}\n",
    "\n",
    "\n",
    "    def build_doc_cleaner(self, lower=True):\n",
    "        \"\"\"\n",
    "        Returns a function that cleans undesirable substrings in a string.\n",
    "        It also lowers the input string if lower=True\n",
    "        \"\"\"\n",
    "        if self.doc_cleaner_func:\n",
    "            return self.doc_cleaner_func\n",
    "        else:\n",
    "            if isinstance(self.doc_cleaner_pattern, self._retype):\n",
    "                clean_doc_pattern = re.compile(self.doc_cleaner_pattern)\n",
    "            else:\n",
    "                clean_doc_pattern = re.compile(self.doc_cleaner_pattern)\n",
    "\n",
    "            if lower:\n",
    "                 return lambda doc: clean_doc_pattern.sub(\" \", doc).lower()\n",
    "            else:\n",
    "                 return lambda doc: clean_doc_pattern.sub(\" \", doc)\n",
    "\n",
    "    def build_tokenizer(self):\n",
    "        \"\"\"Returns a function that splits a string into a sequence of tokens\"\"\"\n",
    "        if self.tokenizer_func:\n",
    "            return self.tokenizer_func\n",
    "        \n",
    "        else:\n",
    "            token_pattern = re.compile(self.token_pattern)\n",
    "            return lambda doc: token_pattern.findall(doc)\n",
    "\n",
    "    def build_word_transformer(self):\n",
    "        \"\"\"Returns a stemmer or lemmatizer if object has any\"\"\"\n",
    "        \n",
    "        if self.word_transformer_func:\n",
    "            return self.word_transformer_func\n",
    "        else:\n",
    "            return lambda word: word\n",
    "        \n",
    "    def tokenize(self, doc):\n",
    "        doc_cleaner      = self.build_doc_cleaner()\n",
    "        doc_tokenizer    = self.build_tokenizer()\n",
    "        doc     = doc_cleaner(doc)\n",
    "        words = doc_tokenizer(doc)\n",
    "            \n",
    "        return words\n",
    "        \n",
    "    def fit(self, X):\n",
    "\n",
    "        assert isinstance(X,list), \"X is expected to be a list of documents\"\n",
    "        \n",
    "        i = 0\n",
    "        word_to_pos = {}\n",
    "        doc_cleaner      = self.build_doc_cleaner()\n",
    "        doc_tokenizer    = self.build_tokenizer()\n",
    "        word_transformer = self.build_word_transformer()\n",
    "        \n",
    "        for x in X:\n",
    "            x     = doc_cleaner(x)\n",
    "            words = doc_tokenizer(x)\n",
    "            for word in words:\n",
    "                word = word_transformer(word)                  \n",
    "                if word not in word_to_pos:\n",
    "                    word_to_pos[word] = i\n",
    "                    i = i + 1\n",
    "\n",
    "        #self.doc_cleaner = doc_cleaner\n",
    "        #self.doc_tokenizer = doc_tokenizer\n",
    "        #self.word_transformer = word_transformer\n",
    "        \n",
    "        self.word_to_pos = word_to_pos            \n",
    "        self.n_features = len(self.word_to_pos)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Implements a transform where counts are created at runtime and kept with a dict\n",
    "        \"\"\"\n",
    "        \n",
    "        doc_cleaner      = self.build_doc_cleaner()\n",
    "        doc_tokenizer    = self.build_tokenizer()\n",
    "        word_transformer = self.build_word_transformer()      \n",
    "        \n",
    "        col_indices = []\n",
    "        row_indices = []\n",
    "        sp_data     = []\n",
    "        \n",
    "        for m, doc in enumerate(X):\n",
    "            doc = doc_cleaner(doc)\n",
    "            word_ind_counter = defaultdict(int)  # document counter for each doc in X\n",
    "            for word in doc_tokenizer(doc):\n",
    "                word = word_transformer(word)   \n",
    "                if word in self.word_to_pos:\n",
    "                    word_ind_counter[self.word_to_pos[word]] +=1 # word count aggregation\n",
    "\n",
    "            sp_data.extend(word_ind_counter.values())\n",
    "            row_indices.extend([m]*len(word_ind_counter))\n",
    "            col_indices.extend(word_ind_counter.keys())\n",
    "\n",
    "        encoded_X = sp.csr_matrix((sp_data,(row_indices,col_indices)),\n",
    "                                   shape=(len(X), self.n_features),\n",
    "                                   dtype=self.dtype)\n",
    "        \n",
    "        return encoded_X\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        word_to_pos = {}\n",
    "        doc_cleaner      = self.build_doc_cleaner()\n",
    "        doc_tokenizer    = self.build_tokenizer()\n",
    "        word_transformer = self.build_word_transformer()\n",
    "        \n",
    "        data = []\n",
    "        ind_col = []\n",
    "        ind_ptr = [0]\n",
    "        \n",
    "        for x in X:\n",
    "            x     = doc_cleaner(x)\n",
    "            words = doc_tokenizer(x)\n",
    "            for word in words:\n",
    "                word = word_transformer(word)                  \n",
    "                index = word_to_pos.setdefault(word, len(word_to_pos))\n",
    "                ind_col.append(index)\n",
    "                data.append(1)\n",
    "            ind_ptr.append(len(ind_col))\n",
    "                           \n",
    "        self.word_to_pos = word_to_pos            \n",
    "        self.n_features = len(self.word_to_pos)\n",
    "        self.word_to_pos = word_to_pos\n",
    "        \n",
    "        #self.doc_cleaner = doc_cleaner\n",
    "        #self.doc_tokenizer = doc_tokenizer\n",
    "        #self.word_transformer = word_transformer\n",
    "        \n",
    "        X_transformed = sp.csr_matrix((data, ind_col, ind_ptr))\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training a document classifier with `SimpleCountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:40.019110Z",
     "start_time": "2021-03-01T12:00:38.615255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SimpleCountVectorizer(doc_cleaner_func=&lt;function &lt;lambda&gt; at 0x0000020EBFAB0D30&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleCountVectorizer</label><div class=\"sk-toggleable__content\"><pre>SimpleCountVectorizer(doc_cleaner_func=&lt;function &lt;lambda&gt; at 0x0000020EBFAB0D30&gt;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SimpleCountVectorizer(doc_cleaner_func=<function <lambda> at 0x0000020EBFAB0D30>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vainilla_count_vectorizer = SimpleCountVectorizer( doc_cleaner_func=lambda doc: doc)\n",
    "vainilla_count_vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:00:42.991870Z",
     "start_time": "2021-03-01T12:00:40.020675Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x155448 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 1899366 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vainilla_count_vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:01:26.948126Z",
     "start_time": "2021-03-01T12:00:45.573090Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mecheste\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.1, max_iter=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.1, max_iter=50)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.1, max_iter=50)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ = vainilla_count_vectorizer.transform(X_train)\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1, max_iter=50)\n",
    "logistic.fit(X_train_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:01:26.999629Z",
     "start_time": "2021-03-01T12:01:26.949918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9847975958988864"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(logistic.predict(X_train_) == y_train.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I) No Stemmer and no doc_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:38:48.175414Z",
     "start_time": "2023-02-22T16:38:48.170122Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vainilla_count_vectorizer = CountVectorizer()\n",
    "\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_0 = sklearn.pipeline.Pipeline([(\"countvectorizer\", vainilla_count_vectorizer),\n",
    "                                         (\"logisticregression\", logistic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:39:53.238353Z",
     "start_time": "2023-02-22T16:39:53.231420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('countvectorizer', CountVectorizer()),\n",
       " ('logisticregression', LogisticRegression(C=0.1))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe_0.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:40:44.274826Z",
     "start_time": "2023-02-22T16:40:42.774106Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mecheste\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression(C=0.1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression(C=0.1))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.1)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
       "                ('logisticregression', LogisticRegression(C=0.1))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe_0.fit(X_train[0:100],y_train[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:41:04.622712Z",
     "start_time": "2023-02-22T16:41:04.593545Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4,  8, 19,  4, 14,  6,  0,  1,\n",
       "        7, 12,  5,  0, 10,  6,  2,  4,  1, 12,  9, 15,  7,  6, 13, 12, 17,\n",
       "       18, 10,  8, 11,  8, 16,  9,  4,  3,  9,  9,  4,  4,  8, 12, 14,  5,\n",
       "       15,  2, 13, 17, 11,  7, 10,  2, 14, 12,  5,  4,  6,  7,  0, 11, 16,\n",
       "        0,  6, 17,  7, 12,  7,  3, 12, 11,  7,  2,  2,  0, 16,  1,  2,  7,\n",
       "        3,  2,  1, 10, 12, 12, 17, 12,  2,  8,  8, 18,  5,  0,  1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe_0.predict(X_train[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:06:09.444697Z",
     "start_time": "2021-03-01T12:05:20.635740Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mecheste\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 38s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_pipe_0.fit(X_train,y_train)\n",
    "y_test_pred  = model_pipe_0.predict(X_test)\n",
    "y_train_pred = model_pipe_0.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:06:13.725989Z",
     "start_time": "2021-03-01T12:06:11.087461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe_0.steps[0][1].transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:06:15.394958Z",
     "start_time": "2021-03-01T12:06:15.392235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.9983206646632491    Accuracy test: 0.791157727031333\n"
     ]
    }
   ],
   "source": [
    "acc_train_0 = np.mean(y_train == y_train_pred)\n",
    "acc_test_0 = np.mean(y_test == y_test_pred)\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_0, acc_test_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II) No stemmer but doc_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:06:24.979150Z",
     "start_time": "2021-03-01T12:06:24.976884Z"
    }
   },
   "outputs": [],
   "source": [
    "simple_count_vectorizer = SimpleCountVectorizer(doc_cleaner_pattern=re.compile(\"[^a-zA-Z]\"))\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_1 = sklearn.pipeline.Pipeline([(\"countvectorizer\", simple_count_vectorizer),\n",
    "                                        (\"logisticregression\", logistic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:06:52.680919Z",
     "start_time": "2021-03-01T12:06:26.928292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 15s\n",
      "Wall time: 1min 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mecheste\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;,\n",
       "                 SimpleCountVectorizer(doc_cleaner_pattern=re.compile(&#x27;[^a-zA-Z]&#x27;))),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression(C=0.1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;,\n",
       "                 SimpleCountVectorizer(doc_cleaner_pattern=re.compile(&#x27;[^a-zA-Z]&#x27;))),\n",
       "                (&#x27;logisticregression&#x27;, LogisticRegression(C=0.1))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleCountVectorizer</label><div class=\"sk-toggleable__content\"><pre>SimpleCountVectorizer(doc_cleaner_pattern=re.compile(&#x27;[^a-zA-Z]&#x27;))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.1)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('countvectorizer',\n",
       "                 SimpleCountVectorizer(doc_cleaner_pattern=re.compile('[^a-zA-Z]'))),\n",
       "                ('logisticregression', LogisticRegression(C=0.1))])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_pipe_1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:07:00.535058Z",
     "start_time": "2021-03-01T12:06:54.528964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.9978787343114726    Accuracy test: 0.7871747211895911\n"
     ]
    }
   ],
   "source": [
    "y_test_pred  = model_pipe_1.predict(X_test)\n",
    "y_train_pred = model_pipe_1.predict(X_train)\n",
    "\n",
    "acc_train_1 = np.mean(y_train == y_train_pred)\n",
    "acc_test_1 = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_1, acc_test_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III) Use a SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:07:06.207058Z",
     "start_time": "2021-03-01T12:07:06.204178Z"
    }
   },
   "outputs": [],
   "source": [
    "simple_count_vectorizer_stemmer = SimpleCountVectorizer(word_transformer_func= SnowballStemmer('english').stem,\n",
    "                                                        doc_cleaner_pattern=re.compile(\"[^a-zA-Z]\"))\n",
    "\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_2 = sklearn.pipeline.Pipeline([(\"countvectorizer\", simple_count_vectorizer_stemmer),\n",
    "                                        (\"logisticregression\", logistic)],\n",
    "                                         )#memory='/Users/Shared/sklearn_mem/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:08:46.522104Z",
     "start_time": "2021-03-01T12:07:08.034092Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mecheste\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.9982322785928938    Accuracy test: 0.790493892724376\n",
      "CPU times: total: 5min 20s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_pipe_2.fit(X_train,y_train)\n",
    "\n",
    "y_test_pred  = model_pipe_2.predict(X_test)\n",
    "y_train_pred = model_pipe_2.predict(X_train)\n",
    "\n",
    "acc_train_2 = np.mean(y_train == y_train_pred)\n",
    "acc_test_2  = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_2, acc_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table with results for each pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:08:52.509004Z",
     "start_time": "2021-03-01T12:08:52.222112Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:08:54.275249Z",
     "start_time": "2021-03-01T12:08:54.270254Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()\n",
    "df_results[\"no clean no stem\"]   = [acc_train_0, acc_test_0]\n",
    "df_results[\"yes clean no stem\"]  = [acc_train_1, acc_test_1]\n",
    "df_results[\"yes clean yes stem\"] = [acc_train_2, acc_test_2]\n",
    "df_results.index=[\"train\",\"test\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:08:56.076926Z",
     "start_time": "2021-03-01T12:08:56.065436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no clean no stem</th>\n",
       "      <th>yes clean no stem</th>\n",
       "      <th>yes clean yes stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.998321</td>\n",
       "      <td>0.997879</td>\n",
       "      <td>0.998232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.791158</td>\n",
       "      <td>0.787175</td>\n",
       "      <td>0.790494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no clean no stem  yes clean no stem  yes clean yes stem\n",
       "train          0.998321           0.997879            0.998232\n",
       "test           0.791158           0.787175            0.790494"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###   Ngram features with Sklearn vectorizer\n",
    "\n",
    "\n",
    "####  IV) Training a document classifier with sklearn `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:08:57.897420Z",
     "start_time": "2021-03-01T12:08:57.895297Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_3 = sklearn.pipeline.Pipeline([(\"countvectorizer\", count_vectorizer),\n",
    "                                          (\"logisticregression\", logistic)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:09:41.755666Z",
     "start_time": "2021-03-01T12:08:59.659210Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mecheste\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.9983206646632491    Accuracy test: 0.791157727031333\n",
      "CPU times: total: 5min 9s\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_pipe_3.fit(X_train,y_train)\n",
    "\n",
    "y_test_pred  = model_pipe_3.predict(X_test)\n",
    "y_train_pred = model_pipe_3.predict(X_train)\n",
    "\n",
    "acc_train_3 = np.mean(y_train == y_train_pred)\n",
    "acc_test_3  = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_3, acc_test_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:09:45.428569Z",
     "start_time": "2021-03-01T12:09:45.426135Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results[\"sklearn countvectorizer\"] = [acc_train_3, acc_test_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:09:47.234189Z",
     "start_time": "2021-03-01T12:09:47.228044Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no clean no stem</th>\n",
       "      <th>yes clean no stem</th>\n",
       "      <th>yes clean yes stem</th>\n",
       "      <th>sklearn countvectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.998321</td>\n",
       "      <td>0.997879</td>\n",
       "      <td>0.998232</td>\n",
       "      <td>0.998321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.791158</td>\n",
       "      <td>0.787175</td>\n",
       "      <td>0.790494</td>\n",
       "      <td>0.791158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no clean no stem  yes clean no stem  yes clean yes stem  \\\n",
       "train          0.998321           0.997879            0.998232   \n",
       "test           0.791158           0.787175            0.790494   \n",
       "\n",
       "       sklearn countvectorizer  \n",
       "train                 0.998321  \n",
       "test                  0.791158  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V) Training a document classifier with sklearn `CountVectorizer` and ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:13:51.960008Z",
     "start_time": "2021-03-01T12:13:51.875889Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,2))\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_4 = sklearn.pipeline.Pipeline([(\"countvectorizer\", count_vectorizer),\n",
    "                                          (\"logisticregression\", logistic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T12:19:03.957095Z",
     "start_time": "2021-03-01T12:13:53.865132Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_pipe_4.fit(X_train,y_train)\n",
    "\n",
    "y_test_pred  = model_pipe_4.predict(X_test)\n",
    "y_train_pred = model_pipe_4.predict(X_train)\n",
    "\n",
    "acc_train_4 = np.mean(y_train == y_train_pred)\n",
    "acc_test_4  = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_4, acc_test_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:20:13.081738Z",
     "start_time": "2021-03-01T13:20:13.078679Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1181803 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 202 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe_4.steps[0][1].transform(X_train[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:20:15.111395Z",
     "start_time": "2021-03-01T13:20:15.108945Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results[\"sklearn countvectorizer 2gram\"] = [acc_train_4, acc_test_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:20:17.001315Z",
     "start_time": "2021-03-01T13:20:16.995125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no clean no stem</th>\n",
       "      <th>yes clean no stem</th>\n",
       "      <th>yes clean yes stem</th>\n",
       "      <th>sklearn countvectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.998321</td>\n",
       "      <td>0.997879</td>\n",
       "      <td>0.998232</td>\n",
       "      <td>0.998321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.791158</td>\n",
       "      <td>0.787175</td>\n",
       "      <td>0.790494</td>\n",
       "      <td>0.791158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no clean no stem  yes clean no stem  yes clean yes stem  \\\n",
       "train          0.998321           0.997879            0.998232   \n",
       "test           0.791158           0.787175            0.790494   \n",
       "\n",
       "       sklearn countvectorizer  \n",
       "train                 0.998321  \n",
       "test                  0.791158  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:20:19.301620Z",
     "start_time": "2021-03-01T13:20:18.913165Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df_results.T[\"test\"].plot(kind=\"barh\", xlim=(0.79,0.83))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SelectKbest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:20:24.297308Z",
     "start_time": "2021-03-01T13:20:24.294423Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,2))\n",
    "feature_selector = SelectKBest(chi2, k = 700000)\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_5 = sklearn.pipeline.Pipeline([(\"count_vectorizer\", count_vectorizer),\n",
    "                                          (\"feature_selector\", feature_selector),\n",
    "                                          (\"logisticregression\", logistic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:23:49.440495Z",
     "start_time": "2021-03-01T13:20:26.279867Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_pipe_5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:23:59.516522Z",
     "start_time": "2021-03-01T13:23:51.327069Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_train = np.mean(model_pipe_5.predict(X_train) == y_train)\n",
    "acc_test = np.mean(model_pipe_5.predict(X_test) == y_test)\n",
    "df_results[\"sklearn countvectorizer 2gram + selection\"] = [acc_train, acc_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:24:01.492699Z",
     "start_time": "2021-03-01T13:24:01.382054Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_results.T[\"test\"].plot(kind=\"barh\", xlim=(0.79,0.83))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:24:03.354145Z",
     "start_time": "2021-03-01T13:24:03.351799Z"
    }
   },
   "outputs": [],
   "source": [
    "simple_count_vectorizer_stemmer = SimpleCountVectorizer(word_transformer_func= SnowballStemmer('english').stem,\n",
    "                                                        doc_cleaner_pattern=re.compile(\"[^a-zA-Z]\"))\n",
    "\n",
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:24:05.228472Z",
     "start_time": "2021-03-01T13:24:05.226390Z"
    }
   },
   "outputs": [],
   "source": [
    "union = sklearn.pipeline.FeatureUnion([(\"simple_count_vectorizer_stemmer\", simple_count_vectorizer_stemmer),\n",
    "                                       (\"count_vectorizer\", count_vectorizer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:24:07.133052Z",
     "start_time": "2021-03-01T13:24:07.130039Z"
    }
   },
   "outputs": [],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "feature_selector = SelectKBest(chi2, k = 700000)\n",
    "model_pipe_6 = sklearn.pipeline.Pipeline([(\"union_vectorizers\", union),\n",
    "                                          (\"feature_selector\", feature_selector),\n",
    "                                          (\"logisticregression\", logistic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:28:00.134193Z",
     "start_time": "2021-03-01T13:24:09.065420Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_pipe_6.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:29:01.299014Z",
     "start_time": "2021-03-01T13:28:02.436371Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_train = np.mean(model_pipe_6.predict(X_train) == y_train)\n",
    "acc_test = np.mean(model_pipe_6.predict(X_test) == y_test)\n",
    "df_results[\"Feature union + selection\"] = [acc_train, acc_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T13:29:03.333324Z",
     "start_time": "2021-03-01T13:29:03.209539Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_results.T[\"test\"].plot(kind=\"barh\", xlim=(0.79,0.83))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
